{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "543_Final_Project",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xprotobeast2/Humpback_Whale_Identification/blob/master/Humpback_Whale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "PZd079AWkWbM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Retrieve and Unpack Data"
      ]
    },
    {
      "metadata": {
        "id": "q3SEEjmGfscv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e973ea0-4aa7-46ef-c6d6-bb6d3264f746"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from time import time\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch import nn\n",
        "import torch.utils.data as torchdata\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "np.random.seed(111)\n",
        "torch.cuda.manual_seed_all(111)\n",
        "torch.manual_seed(111)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f44a71b5510>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "e8RYCCi_Pmqg",
        "colab_type": "code",
        "outputId": "dc5e62e6-4ed6-45e6-d8f8-89dac61fa489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "a = torch.Tensor([1]).cuda()\n",
        "print(a)\n",
        "\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1.], device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "YsAeB9pbjPNZ",
        "colab_type": "code",
        "outputId": "1070955d-be2a-4a5b-9d79-db66554d1923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "!mkdir .kaggle\n",
        "!mkdir data\n",
        "\n",
        "\n",
        "\n",
        "token = {\"username\":\"ramg95\",\"key\":\"f713f68e67eed775abc23149ec728820\"}\n",
        "\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)\n",
        "    \n",
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle config set -n path -v \"/content/data\"\n",
        "!kaggle competitions download -c humpback-whale-identification\n",
        "\n",
        "!unzip -q data/competitions/humpback-whale-identification/test.zip -d data/test/\n",
        "!unzip -q data/competitions/humpback-whale-identification/train.zip -d data/train/\n",
        "!mv data/competitions/humpback-whale-identification/*.csv data/\n",
        "!rm -rf data/competitions"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.3.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.21.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (3.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.8)\n",
            "Requirement already satisfied: text-unidecode==1.2 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.2)\n",
            "mkdir: cannot create directory ‘.kaggle’: File exists\n",
            "mkdir: cannot create directory ‘data’: File exists\n",
            "- path is now set to: /content/data\n",
            "Downloading sample_submission.csv to /content/data/competitions/humpback-whale-identification\n",
            "  0% 0.00/498k [00:00<?, ?B/s]\n",
            "100% 498k/498k [00:00<00:00, 69.1MB/s]\n",
            "Downloading train.csv to /content/data/competitions/humpback-whale-identification\n",
            "  0% 0.00/594k [00:00<?, ?B/s]\n",
            "100% 594k/594k [00:00<00:00, 84.4MB/s]\n",
            "Downloading test.zip to /content/data/competitions/humpback-whale-identification\n",
            " 99% 1.34G/1.35G [00:25<00:00, 58.5MB/s]\n",
            "100% 1.35G/1.35G [00:25<00:00, 56.5MB/s]\n",
            "Downloading train.zip to /content/data/competitions/humpback-whale-identification\n",
            "100% 4.15G/4.16G [01:03<00:00, 70.6MB/s]\n",
            "100% 4.16G/4.16G [01:03<00:00, 69.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "adhJYZn7u4UE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Humpback_Whale_Dataset(torchdata.Dataset):\n",
        "    def __init__(self, data_root , fold='train', known_only=False, validation_fraction=0.1, transforms=None):\n",
        "        \n",
        "        self.root = data_root\n",
        "        fold = fold.lower()\n",
        "\n",
        "        self.train = False\n",
        "        self.test = False\n",
        "        self.val = False\n",
        "\n",
        "        if fold == \"train\":\n",
        "            self.train = True\n",
        "        elif fold == \"test\":\n",
        "            self.test = True\n",
        "        elif fold == \"val\":\n",
        "            self.val = True\n",
        "        else:\n",
        "            raise RuntimeError(\"Not train-val-test\")\n",
        "        \n",
        "        # Load data based on fold name\n",
        "        if self.train or self.val:\n",
        "            data_info = pd.read_csv(data_root+'train.csv', header=0)\n",
        "            \n",
        "            if known_only:\n",
        "                data_info = data_info[data_info.Id != 'new_whale']\n",
        "            \n",
        "            \n",
        "            self.images = data_info.Image.values\n",
        "            self.labels = data_info.Id.values\n",
        "\n",
        "            # Now make a mask for every nth image (training and validation)\n",
        "            step = int(1/validation_fraction)\n",
        "            p = np.arange(0, len(self.images), step)\n",
        "            val_mask = np.zeros(len(self.images), dtype=bool)\n",
        "            train_mask = np.ones(len(self.images), dtype=bool)\n",
        "            train_mask[p] = False\n",
        "            val_mask[p] = True\n",
        "            \n",
        "            # set data and label values\n",
        "            self.data = np.array(self.images[train_mask]) if self.train else np.array(self.images[val_mask])\n",
        "            self.labels = np.array(self.labels[train_mask]) if self.train else np.array(self.labels[val_mask])\n",
        "            self.data_path = data_root+'train/'\n",
        "            \n",
        "        elif self.test:\n",
        "            \n",
        "            # Directly load names from os.listdir\n",
        "            self.data = np.array(os.listdir(data_root+'test/'))\n",
        "            self.labels = np.zeros(len(self.data))\n",
        "            self.data_path = data_root+'test/'\n",
        "        \n",
        "        self.length = len(self.data)\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        img_file = os.path.join(self.data_path, self.data[index])\n",
        "        image = Image.open(img_file)\n",
        "        label = self.labels[index]\n",
        "        \n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        return (image, label)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "    \n",
        "    def __repr__(self):\n",
        "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
        "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
        "        tmp = 'train' if self.train is True else 'test'\n",
        "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
        "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
        "        tmp = '    Transforms (if any): '\n",
        "        fmt_str += '{0}{1}\\n'.format(tmp, self.transforms.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))       \n",
        "        return fmt_str\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xnv6YPnHn_dj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DATA_ROOT = \"data/\"\n",
        "TRAIN_BS = 16\n",
        "TEST_BS = 128\n",
        "IMAGE_RESIZED_DIM = 100\n",
        "\n",
        "transform = transforms.Compose([\n",
        "        transforms.Grayscale(1),\n",
        "        transforms.Resize((IMAGE_RESIZED_DIM,IMAGE_RESIZED_DIM)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "train_set = Humpback_Whale_Dataset(data_root=DATA_ROOT, \n",
        "                                   fold='train', \n",
        "                                   known_only=True,\n",
        "                                   validation_fraction = 0.2,\n",
        "                                   transforms=transform)\n",
        "val_set = Humpback_Whale_Dataset(data_root=DATA_ROOT, \n",
        "                                   fold='val', \n",
        "                                   validation_fraction = 0.2,\n",
        "                                   known_only=True,\n",
        "                                   transforms=transform)\n",
        "test_set = Humpback_Whale_Dataset(data_root=DATA_ROOT, \n",
        "                                   fold='test',\n",
        "                                   known_only=True,\n",
        "                                   transforms=transform)\n",
        "\n",
        "trainloader = torchdata.DataLoader(train_set, batch_size=TRAIN_BS,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "valloader = torchdata.DataLoader(val_set, batch_size=TEST_BS,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "testloader = torchdata.DataLoader(test_set, batch_size=TEST_BS,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-tsh6nkSjRpm",
        "colab_type": "code",
        "outputId": "c1261b4f-6217-4ac8-8a3d-476bacde4866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "trainloader.dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset Humpback_Whale_Dataset\n",
              "    Number of datapoints: 12557\n",
              "    Split: train\n",
              "    Root Location: data/\n",
              "    Transforms (if any): Compose(\n",
              "                             Grayscale(num_output_channels=1)\n",
              "                             Resize(size=(100, 100), interpolation=PIL.Image.BILINEAR)\n",
              "                             ToTensor()\n",
              "                         )"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "58RJajw8jRtE",
        "colab_type": "code",
        "outputId": "6912867c-ed12-4af2-94ab-bf191a799a27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "valloader.dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset Humpback_Whale_Dataset\n",
              "    Number of datapoints: 3140\n",
              "    Split: test\n",
              "    Root Location: data/\n",
              "    Transforms (if any): Compose(\n",
              "                             Grayscale(num_output_channels=1)\n",
              "                             Resize(size=(100, 100), interpolation=PIL.Image.BILINEAR)\n",
              "                             ToTensor()\n",
              "                         )"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "h409nwq4jR6B",
        "colab_type": "code",
        "outputId": "f1a59798-830c-461a-a72a-cd8c7c7c5b50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "testloader.dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset Humpback_Whale_Dataset\n",
              "    Number of datapoints: 7960\n",
              "    Split: test\n",
              "    Root Location: data/\n",
              "    Transforms (if any): Compose(\n",
              "                             Grayscale(num_output_channels=1)\n",
              "                             Resize(size=(100, 100), interpolation=PIL.Image.BILINEAR)\n",
              "                             ToTensor()\n",
              "                         )"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5tBQPF0oNZ0u",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(10,1)\n",
        "fig.set_size_inches(10,30)\n",
        "for i in range(10):\n",
        "    ax[i].imshow(plt.imread(DATA_ROOT+'train/'+known_whales.Image.iloc[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hjcES3X0SRxH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model Pipelines"
      ]
    },
    {
      "metadata": {
        "id": "MHudju9Z8Yu9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def validate(model, valloader, has_gpu):\n",
        "    \n",
        "    correct = total = 0.0\n",
        "    class_correct = list(0. for i in range(TOTAL_CLASSES))\n",
        "    class_total = list(0. for i in range(TOTAL_CLASSES))\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in valloader:\n",
        "            if is_gpu:\n",
        "                images = images.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "            # Predict and compute accuracy    \n",
        "            outputs = model(images)\n",
        "            predicted = torch.argmax(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum()\n",
        "\n",
        "\n",
        "            c = (predicted == labels).squeeze()\n",
        "            for i in range(len(labels)):\n",
        "                label = labels[i]\n",
        "                class_correct[label] += c[i]\n",
        "                class_total[label] += 1\n",
        "\n",
        "    class_accuracy = 100 * np.divide(class_correct, class_total)\n",
        "    return 100*correct/total, class_accuracy\n",
        "\n",
        "def train(model, optimizer, loss_function, trainloader, valloader,  has_gpu=True, num_epochs):\n",
        "    \n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        correct = total = running_loss = 0.0\n",
        "        for images, labels in trainloader:\n",
        "            # Train here\n",
        "            if has_gpu:\n",
        "                images = images.cuda()\n",
        "                labels = labels.cuda()\n",
        "            \n",
        "            # Forward, and loss compute\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            \n",
        "            \n",
        "            # Calculate gradients, update parameters\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Update train accuracy values\n",
        "            correct += (torch.argmax(outputs, 1) == labels).sum().float()\n",
        "            total += labels.size(0)\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            \n",
        "            \n",
        "        print(\"Took %f seconds\"%(time() - a))\n",
        "        print('Accuracy of the network on the train images: %f %%' % ((correct/total)*100))\n",
        "        print('Accuracy of the network on the val images: %d %%' % (val_accuracy))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EzGKFannICgg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gnb.score(X_train[:1000], Y_train[:1000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DN-XEQZ200KR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train.shape\n",
        "Y_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eX5i3ZPstJKO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Techniques to apply to data:\n",
        "\n",
        "Problem: Data is not the same size\n",
        "1. Vector Quantization\n",
        "    \n",
        "\n",
        "*   Pick a 2-D patch_size, and chop image up into patches with/without overlap. \n",
        "*   Make a \n",
        "\n",
        "\n",
        "2. Downsampling\n",
        "3. Upsampling\n",
        "\n",
        "\n",
        "Problem : High dimension , Solution: Dimensionality Reduction\n",
        "1. PCA\n",
        "2. Autoencoders?\n",
        "\n",
        "Problem: Data imbalance Solution: Augmentation\n",
        "1. Data imbalance is when we have lots of data for one class, but only like 1 example for others. The way we handle this is either to use Machine learning models with data weighting. One common way is oversampling/augmentation.\n",
        "\n",
        "we can make copies with modifications of the examples in the classes with lower representation. "
      ]
    },
    {
      "metadata": {
        "id": "sZOSd7v9LO-h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1MpxL9HMT4c5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TODO\n",
        "\n",
        "\n",
        "\n",
        "1.   Train SVM to recognize a unique whale. Train num_classes SVMs.\n",
        "2.   Random Forest Classifiers\n",
        "3.   CNN based architectures\n",
        "\n",
        "\n",
        "*   Model from scratch\n",
        "*   Pretrained/Transfer Learning\n",
        "\n",
        "\n",
        "MOST PROMISING PIPELINE:\n",
        "\n",
        "https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "jSB6QujLT58x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}