\section{Discussion}

We set out to make the leader board in this competition, but this turned out to be an ambitious goal. We settled with seeing how various vision methods perform on this task, and documenting our results. Here we discuss possible explanations and do a rough analysis of our results. 

\subsection{SIFT/ORB and Nearest Neighbors}

This method suffered from several flaws. The poor performance can be attributed to many factors. Firstly, we lost a lot of data when we down-sampled the images to a size small enough to compute key points, matches, and distances in a feasible time period. We estimated that at full resolution, it would take more than 24 straight hours to just compute matches between all pairs of test and train images. Secondly, our distance metric, average Hamming distance across all matches, may be poor. Finally, nearest neighbor classifiers tend to underfit and suffer from the \textit{curse of dimension} even with down-sampled images. This combination of requirements is yet another thing that points to the necessity for deep learning models.
We didn't see any benefit by increasing the value of K, as for the non-augmented dataset, some training classes have only one example.

\subsection{Transfer Learning}

Transfer learning using even state-of-the-art pretrained networks has limitations; on the whale detection problem posed in this project, we achieved only mediocre success. The biggest reason for this is that ResNets which are trained on ImageNet, although well-trained in recognizing objects in images in general, are not trained for recognizing minute differences in the unique shapes of whale flukes, which is the main information in this dataset's images. Significant noise is caused by the waves in the water, similar to how it's picked up by the keypoint detectors discussed above. A lot of successful attempts at this whale classification task thus use bounding boxes to limit computational analysis of neural nets or other models to only the whale flukes.

The various preprocessing techniques we employed were educational in that they demonstrate additional preprocessing if not done correctly, or if not trained enough can actually result in worse results, as is evidenced by our high train and validation accuracies compared to the test accuracy in several configurations. The various half-frozen network configurations we tried yielded worse results than just training the last fully-connected (FC) layer, or fine-tuning the entire network.

\subsection{Bounding Box Model}

We were not able to get a significant increase in validation and testing accuracy with this model. We need to more accurately understand how to combine the bounding box learning model with other models in our pipeline. The standard deep learning interfaces such as what to do when our model is overfitting, and what to do when it is underfitting were not enough to properly apply this model. 

\subsection{Future Work}

The key to this project, we found out closer to the deadline, was one-shot learning, and an effective representation of each whale. On representation, we needed to concentrate more on the pattern of contours on the edges of the whale tail flukes, rather than trying to automatically learn those features from scratch. A vector representation as in \cite{weideman2017integral} is low-dimensional enough to apply a strong learning technique for one-shot learning.\\

One such technique is Siamese Networks. Such a network consists of a \textit{head network} and a \textit{branch network}. The \textit{branch network} transforms the input image into a vector of features describing the whale - minimizing the distance between representations between examples of the same class and the other tries to maximize the distance between representations of other classes. The \textit{head network} then is used to compare the feature vectors which are an output of the branch model to identify whales. We did not have the time nor the knowledge to complete this attempt for this project, but we will do so in the future.